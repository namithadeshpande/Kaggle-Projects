{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1 align='center'><u>Spam Detection</u></h1>\n\n","metadata":{"id":"_mJQGbTFOCAx"}},{"cell_type":"code","source":"!pip install -U spacy","metadata":{"id":"Z8KpPS9Cl9tD","outputId":"7fd6b40d-5d5f-4bfa-98df-db3efc191826","execution":{"iopub.status.busy":"2022-02-24T17:24:11.459584Z","iopub.execute_input":"2022-02-24T17:24:11.459903Z","iopub.status.idle":"2022-02-24T17:24:19.444078Z","shell.execute_reply.started":"2022-02-24T17:24:11.459868Z","shell.execute_reply":"2022-02-24T17:24:19.442946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install pyspellchecker","metadata":{"id":"_fSmnBkyCZd5","outputId":"e6b0b7aa-7811-4a7b-8454-46bc2f5e4946","execution":{"iopub.status.busy":"2022-02-24T17:24:19.446483Z","iopub.execute_input":"2022-02-24T17:24:19.446975Z","iopub.status.idle":"2022-02-24T17:24:26.551632Z","shell.execute_reply.started":"2022-02-24T17:24:19.446927Z","shell.execute_reply":"2022-02-24T17:24:26.550546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install bs4","metadata":{"id":"lleWVrW2fJRB","outputId":"d5ebc28d-d704-4fbf-b3d8-390e0075883f","execution":{"iopub.status.busy":"2022-02-24T17:24:26.553884Z","iopub.execute_input":"2022-02-24T17:24:26.55464Z","iopub.status.idle":"2022-02-24T17:24:33.823414Z","shell.execute_reply.started":"2022-02-24T17:24:26.554589Z","shell.execute_reply":"2022-02-24T17:24:33.822729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import Libraries\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\nimport textwrap as tw\nimport matplotlib.pyplot as plt\n\n# learning Curves\nfrom sklearn.model_selection import learning_curve\n\n# save and load models\nimport joblib\n\nimport re\nfrom bs4 import BeautifulSoup\n\nfrom scipy.sparse import hstack\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer, TfidfVectorizer\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report\n\nimport spacy\nfrom spacy.matcher import Matcher\nfrom spacy.tokens import Token\n\n\nimport nltk\nfrom nltk.stem.porter import PorterStemmer\n\n\nfrom sklearn.model_selection import train_test_split\nfrom collections import Counter\nfrom xgboost import XGBClassifier\nfrom sklearn import metrics\nfrom spellchecker import SpellChecker","metadata":{"id":"2vzoDb7CltyX","execution":{"iopub.status.busy":"2022-02-24T17:24:33.825458Z","iopub.execute_input":"2022-02-24T17:24:33.825811Z","iopub.status.idle":"2022-02-24T17:24:33.834455Z","shell.execute_reply.started":"2022-02-24T17:24:33.82577Z","shell.execute_reply":"2022-02-24T17:24:33.833814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2022-02-24T17:24:33.837363Z","iopub.execute_input":"2022-02-24T17:24:33.83766Z","iopub.status.idle":"2022-02-24T17:24:33.85539Z","shell.execute_reply.started":"2022-02-24T17:24:33.837627Z","shell.execute_reply":"2022-02-24T17:24:33.854619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_folder = Path('../input/sms-spam-collection-dataset')","metadata":{"id":"wMInbDePmsRC","execution":{"iopub.status.busy":"2022-02-24T17:24:33.856361Z","iopub.execute_input":"2022-02-24T17:24:33.856603Z","iopub.status.idle":"2022-02-24T17:24:33.866552Z","shell.execute_reply.started":"2022-02-24T17:24:33.856574Z","shell.execute_reply":"2022-02-24T17:24:33.865894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python -m spacy download 'en_core_web_sm'","metadata":{"id":"L8t3Ye3SXmoT","outputId":"9c4db7e5-be44-41bb-d7e2-0274aeafa604","execution":{"iopub.status.busy":"2022-02-24T17:24:33.867812Z","iopub.execute_input":"2022-02-24T17:24:33.868302Z","iopub.status.idle":"2022-02-24T17:24:49.323476Z","shell.execute_reply.started":"2022-02-24T17:24:33.868266Z","shell.execute_reply":"2022-02-24T17:24:49.322224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nlp = spacy.load('en_core_web_sm')","metadata":{"id":"0Y2uAA66m_DD","execution":{"iopub.status.busy":"2022-02-24T17:24:49.325308Z","iopub.execute_input":"2022-02-24T17:24:49.325591Z","iopub.status.idle":"2022-02-24T17:24:49.846077Z","shell.execute_reply.started":"2022-02-24T17:24:49.325558Z","shell.execute_reply":"2022-02-24T17:24:49.845125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load the dataset  \n\n\n\n","metadata":{"id":"cXeG6ZQ4OVDj"}},{"cell_type":"code","source":"# location of data file\nspam_file = data_folder / 'spam.csv'\n\n# creating Pandas Dataframe\ndf = pd.read_csv(spam_file, index_col=0,encoding = 'ISO-8859-1')","metadata":{"id":"WIXm9xz8kBQd","execution":{"iopub.status.busy":"2022-02-24T17:24:49.847718Z","iopub.execute_input":"2022-02-24T17:24:49.847992Z","iopub.status.idle":"2022-02-24T17:24:49.872748Z","shell.execute_reply.started":"2022-02-24T17:24:49.847954Z","shell.execute_reply":"2022-02-24T17:24:49.871875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print shape of the dataset\nprint(f'Shape of data set is : {df.shape}')","metadata":{"id":"kiBawPgdqeNN","outputId":"b50a5b84-f981-469d-9e38-5cc708d38e20","execution":{"iopub.status.busy":"2022-02-24T17:24:49.874131Z","iopub.execute_input":"2022-02-24T17:24:49.874331Z","iopub.status.idle":"2022-02-24T17:24:49.878785Z","shell.execute_reply.started":"2022-02-24T17:24:49.874307Z","shell.execute_reply":"2022-02-24T17:24:49.87806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop(columns = ['Unnamed: 2','Unnamed: 3', 'Unnamed: 4'], inplace=True)\ndf.reset_index(inplace=True)\ndf.rename(columns={'v1':'label', 'v2':'text'},inplace=True)\n\n# Printing basic info\ndf.info()","metadata":{"id":"lp_Ne2NYwkti","outputId":"f48ee906-ea96-4869-e8dd-f97f06729ba3","execution":{"iopub.status.busy":"2022-02-24T17:24:49.87981Z","iopub.execute_input":"2022-02-24T17:24:49.880006Z","iopub.status.idle":"2022-02-24T17:24:49.901202Z","shell.execute_reply.started":"2022-02-24T17:24:49.879981Z","shell.execute_reply":"2022-02-24T17:24:49.90052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking distribution of class labels for train dataset\ndf['label'].value_counts(normalize=True)","metadata":{"id":"sa0EazNurudw","outputId":"25825f53-4ddf-4970-aa06-5c7f9aabf7c3","execution":{"iopub.status.busy":"2022-02-24T17:24:49.902268Z","iopub.execute_input":"2022-02-24T17:24:49.902841Z","iopub.status.idle":"2022-02-24T17:24:49.910461Z","shell.execute_reply.started":"2022-02-24T17:24:49.902806Z","shell.execute_reply":"2022-02-24T17:24:49.909649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Metric for evaluating model\n","metadata":{"id":"pNdWCViagO4F"}},{"cell_type":"code","source":"# Creating a scorer for F2 score so that we can given an emphases on the minority class predictions i.e higher recall\nfrom sklearn.metrics import fbeta_score, make_scorer\nfscore = make_scorer(fbeta_score, beta=0.5)\nfscore","metadata":{"id":"dTjftr2jr3-F","outputId":"bdf169e8-655e-4640-807d-9762e8b6e4ab","execution":{"iopub.status.busy":"2022-02-24T17:24:49.911618Z","iopub.execute_input":"2022-02-24T17:24:49.912204Z","iopub.status.idle":"2022-02-24T17:24:49.923218Z","shell.execute_reply.started":"2022-02-24T17:24:49.912164Z","shell.execute_reply":"2022-02-24T17:24:49.922615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* As seen in the previous step, we have a heavily imbalanced dataset. Hence, using accuracy as an evaluation metric doesn't bear good results.\n\n* **Precision** and **recall** are used to take care of false positive and false negative rates. We have F-score which is the harmonic mean of precision and recall and gives equal weightage to both. \n\n* In this case, I will be using a derivative of F-score which is a **Fbeta-measure**. It is used when both precision and recall are to be considered with a higher weightage for one of them. \n\n* In our dataset, I am more concerned about marking a non-spam email as a spam email which causes **False Positive**. So more weightage is to be given to **Precision**.\n\n* I will be using **beta=0.5** in this exercise. So it will be F0.5 measure.","metadata":{"id":"x3VxdQtNsWzD"}},{"cell_type":"markdown","source":"# Classification Pipelines\n\n    1. Featurization (TF-IDF) + Feature Engineering + ML Model pipeline\n\n**Requirements:** \n\n1. Using XgBoost model for the classification and tuning the **XGBoost for imbalanced dataset** (If you have never used XGBoost before , here is the link on XGBoost tutorial for imbalanced data: https://machinelearningmastery.com/xgboost-for-imbalanced-classification/).\n\n2. For feature engineering, I will Count of following  (Nouns, ProperNouns, AUX, VERBS, Adjectives, named entities, spelling mistakes (see the link on how to get spelling mistakes https://pypi.org/project/pyspellchecker/). \n\n3. For Sparse embeddings I will use **tfidf vectorization**. We should choose appropriate parameters e.g. min_df, max_df, max_faetures, n-grams etc.)","metadata":{"id":"rH8_mvGhiThV"}},{"cell_type":"markdown","source":"## Sampling and Train-Test Split","metadata":{"id":"8HO-ifjW0q0d"}},{"cell_type":"code","source":"df['label'] = df['label'].map({'spam':1, 'ham':0}).astype(int)","metadata":{"id":"QIJZrWEDr9aZ","execution":{"iopub.status.busy":"2022-02-24T17:24:49.926352Z","iopub.execute_input":"2022-02-24T17:24:49.926689Z","iopub.status.idle":"2022-02-24T17:24:49.936654Z","shell.execute_reply.started":"2022-02-24T17:24:49.926656Z","shell.execute_reply":"2022-02-24T17:24:49.935621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = df['text'].values\ny = df['label'].values\nX_train, X_test, y_train, y_test = train_test_split(X,y, random_state=0)\nprint(f'X_train: {X_train.shape} y_train: {y_train.shape}')\nprint(f'X_test: {X_test.shape} y_test: {y_test.shape}')","metadata":{"id":"Knl_ZSHgvux4","outputId":"8e739cac-dd50-4baf-c4a5-e722a2ec4470","execution":{"iopub.status.busy":"2022-02-24T17:24:49.93794Z","iopub.execute_input":"2022-02-24T17:24:49.938172Z","iopub.status.idle":"2022-02-24T17:24:49.954322Z","shell.execute_reply.started":"2022-02-24T17:24:49.938144Z","shell.execute_reply":"2022-02-24T17:24:49.953458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Custom Classes ","metadata":{"id":"qi9CVtJ1XycF"}},{"cell_type":"code","source":"class SpacyPreprocessor(BaseEstimator, TransformerMixin):\n    '''\n    model : spacy model to be used. For example fpr english language we can specify: en_core_web_sm\n\n    '''\n    np.random.seed(0)\n    def __init__(self, model, batch_size = 64, lammetize=True, lower=True, remove_stop=True, \n                 remove_punct=True, remove_email=True, remove_url=True, remove_num=False, stemming = False,\n                 add_user_mention_prefix=True, remove_hashtag_prefix=False):\n        self.model = model\n        self.batch_size = batch_size\n        self.remove_stop = remove_stop\n        self.remove_punct = remove_punct\n        self.remove_num = remove_num\n        self.remove_url = remove_url\n        self.remove_email = remove_email\n        self.lammetize = lammetize\n        self.lower = lower\n        self.stemming = stemming\n        self.add_user_mention_prefix = add_user_mention_prefix\n        self.remove_hashtag_prefix = remove_hashtag_prefix\n\n # helpfer functions for basic cleaning \n\n    def basic_clean(self, text):\n        \n        '''\n        This fuction removes HTML tags from text\n        '''\n        if (bool(BeautifulSoup(text, \"html.parser\").find())==True):         \n            soup = BeautifulSoup(text, \"html.parser\")\n            text = soup.get_text()\n        else:\n            pass\n        return re.sub(r'[\\n\\r]',' ', text) \n\n    # helper function for pre-processing with spacy and Porter Stemmer\n    \n    def spacy_preprocessor(self,texts):\n\n        final_result = []\n        nlp = spacy.load(self.model)\n        if self.lammetize:   \n          disabled = nlp.select_pipes(disable= [ 'parser', 'ner'])\n        else:\n          disabled = nlp.select_pipes(disable= ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner'])\n        \n        ## Add @ as a prefix so that we can separate the word from @ \n        prefixes = list(nlp.Defaults.prefixes)\n\n        if self.add_user_mention_prefix:\n            prefixes += ['@']\n\n        ## Remove # as a prefix so that we can keep hashtags and words together\n        if self.remove_hashtag_prefix:\n            prefixes.remove(r'#')\n\n        prefix_regex = spacy.util.compile_prefix_regex(prefixes)\n        nlp.tokenizer.prefix_search = prefix_regex.search\n\n        matcher = Matcher(nlp.vocab)\n        if self.remove_stop:\n            matcher.add(\"stop_words\", [[{\"is_stop\" : True}]])\n        if self.remove_punct:\n            matcher.add(\"punctuation\",[ [{\"is_punct\": True}]])\n        if self.remove_num:\n            matcher.add(\"numbers\", [[{\"like_num\": True}]])\n        if self.remove_url:\n            matcher.add(\"urls\", [[{\"like_url\": True}]])\n        if self.remove_email:\n            matcher.add(\"emails\", [[{\"like_email\": True}]])\n            \n        Token.set_extension('is_remove', default=False, force=True)\n\n        cleaned_text = []\n        for doc in nlp.pipe(texts,batch_size=self.batch_size ):\n            matches = matcher(doc)\n            for _, start, end in matches:\n                for token in doc[start:end]:\n                    token._.is_remove =True\n                    \n            if self.lammetize:                     \n                text = ' '.join(token.lemma_ for token in doc if (token._.is_remove==False))\n            elif self.stemming:\n                text = ' '.join(PorterStemmer().stem(token.text) for token in doc if (token._.is_remove==False))\n            else:\n                text = ' '.join(token.text for token in doc if (token._.is_remove==False))\n                                   \n            if self.lower:\n                text=text.lower()\n            cleaned_text.append(text)\n        return cleaned_text\n\n    def fit(self, X,y=None):\n        return self\n\n    def transform(self, X, y=None):\n        try:\n            if str(type(X)) not in [\"<class 'list'>\",\"<class 'numpy.ndarray'>\"]:\n                raise Exception('Expected list or numpy array got {}'.format(type(X)))\n            x_clean = [self.basic_clean(text) for text in X]\n            x_clean_final = self.spacy_preprocessor(x_clean)\n            return x_clean_final\n        except Exception as error:\n            print('An exception occured: ' + repr(error))","metadata":{"id":"4pZRcfFKOXjm","execution":{"iopub.status.busy":"2022-02-24T17:24:49.956006Z","iopub.execute_input":"2022-02-24T17:24:49.956239Z","iopub.status.idle":"2022-02-24T17:24:49.977161Z","shell.execute_reply.started":"2022-02-24T17:24:49.956212Z","shell.execute_reply":"2022-02-24T17:24:49.976432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_learning_curve(\n    estimator,\n    title,\n    X,\n    y,\n    axes=None,\n    ylim=None,\n    cv=None,\n    n_jobs=None,\n    train_sizes=np.linspace(0.1, 1.0, 5),\n):\n    \"\"\"\n    Generate 3 plots: the test and training learning curve, the training\n    samples vs fit times curve, the fit times vs score curve.\n\n    Parameters\n    ----------\n    estimator : estimator instance\n        An estimator instance implementing `fit` and `predict` methods which\n        will be cloned for each validation.\n\n    title : str\n        Title for the chart.\n\n    X : array-like of shape (n_samples, n_features)\n        Training vector, where ``n_samples`` is the number of samples and\n        ``n_features`` is the number of features.\n\n    y : array-like of shape (n_samples) or (n_samples, n_features)\n        Target relative to ``X`` for classification or regression;\n        None for unsupervised learning.\n\n    axes : array-like of shape (3,), default=None\n        Axes to use for plotting the curves.\n\n    ylim : tuple of shape (2,), default=None\n        Defines minimum and maximum y-values plotted, e.g. (ymin, ymax).\n\n    cv : int, cross-validation generator or an iterable, default=None\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n\n          - None, to use the default 5-fold cross-validation,\n          - integer, to specify the number of folds.\n          - :term:`CV splitter`,\n          - An iterable yielding (train, test) splits as arrays of indices.\n\n        For integer/None inputs, if ``y`` is binary or multiclass,\n        :class:`StratifiedKFold` used. If the estimator is not a classifier\n        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validators that can be used here.\n\n    n_jobs : int or None, default=None\n        Number of jobs to run in parallel.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    train_sizes : array-like of shape (n_ticks,)\n        Relative or absolute numbers of training examples that will be used to\n        generate the learning curve. If the ``dtype`` is float, it is regarded\n        as a fraction of the maximum size of the training set (that is\n        determined by the selected validation method), i.e. it has to be within\n        (0, 1]. Otherwise it is interpreted as absolute sizes of the training\n        sets. Note that for classification the number of samples usually have\n        to be big enough to contain at least one sample from each class.\n        (default: np.linspace(0.1, 1.0, 5))\n    \"\"\"\n    if axes is None:\n        _, axes = plt.subplots(1, 3, figsize=(20, 5))\n\n    axes[0].set_title(title)\n    if ylim is not None:\n        axes[0].set_ylim(*ylim)\n    axes[0].set_xlabel(\"Training examples\")\n    axes[0].set_ylabel(\"Score\")\n\n    train_sizes, train_scores, test_scores, fit_times, _ = learning_curve(\n        estimator,\n        X,\n        y,\n        cv=cv,\n        n_jobs=n_jobs,\n        train_sizes=train_sizes,\n        return_times=True,\n    )\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    fit_times_mean = np.mean(fit_times, axis=1)\n    fit_times_std = np.std(fit_times, axis=1)\n\n    # Plot learning curve\n    axes[0].grid()\n    axes[0].fill_between(\n        train_sizes,\n        train_scores_mean - train_scores_std,\n        train_scores_mean + train_scores_std,\n        alpha=0.1,\n        color=\"r\",\n    )\n    axes[0].fill_between(\n        train_sizes,\n        test_scores_mean - test_scores_std,\n        test_scores_mean + test_scores_std,\n        alpha=0.1,\n        color=\"g\",\n    )\n    axes[0].plot(\n        train_sizes, train_scores_mean, \"o-\", color=\"r\", label=\"Training score\"\n    )\n    axes[0].plot(\n        train_sizes, test_scores_mean, \"o-\", color=\"g\", label=\"Cross-validation score\"\n    )\n    axes[0].legend(loc=\"best\")\n\n    # Plot n_samples vs fit_times\n    axes[1].grid()\n    axes[1].plot(train_sizes, fit_times_mean, \"o-\")\n    axes[1].fill_between(\n        train_sizes,\n        fit_times_mean - fit_times_std,\n        fit_times_mean + fit_times_std,\n        alpha=0.1,\n    )\n    axes[1].set_xlabel(\"Training examples\")\n    axes[1].set_ylabel(\"fit_times\")\n    axes[1].set_title(\"Scalability of the model\")\n\n    # Plot fit_time vs score\n    fit_time_argsort = fit_times_mean.argsort()\n    fit_time_sorted = fit_times_mean[fit_time_argsort]\n    test_scores_mean_sorted = test_scores_mean[fit_time_argsort]\n    test_scores_std_sorted = test_scores_std[fit_time_argsort]\n    axes[2].grid()\n    axes[2].plot(fit_time_sorted, test_scores_mean_sorted, \"o-\")\n    axes[2].fill_between(\n        fit_time_sorted,\n        test_scores_mean_sorted - test_scores_std_sorted,\n        test_scores_mean_sorted + test_scores_std_sorted,\n        alpha=0.1,\n    )\n    axes[2].set_xlabel(\"fit_times\")\n    axes[2].set_ylabel(\"Score\")\n    axes[2].set_title(\"Performance of the model\")\n\n    return plt","metadata":{"id":"7vu7NyyXOAzc","execution":{"iopub.status.busy":"2022-02-24T17:24:49.979324Z","iopub.execute_input":"2022-02-24T17:24:49.979637Z","iopub.status.idle":"2022-02-24T17:24:50.000664Z","shell.execute_reply.started":"2022-02-24T17:24:49.979609Z","shell.execute_reply":"2022-02-24T17:24:49.999792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ManualFeatures(TransformerMixin, BaseEstimator):\n    \n    def __init__(self, spacy_model, pos_features = True, ner_features = True, count_features = True):\n        \n        self.spacy_model = spacy_model\n        self.pos_features = pos_features\n        self.ner_features = ner_features\n        self.count_features = count_features    \n    \n        \n    # Define some helper functions\n    def get_pos_features(self, cleaned_text):\n        nlp = spacy.load(self.spacy_model)\n        noun_count = []\n        aux_count = []\n        verb_count = []\n        adj_count =[]\n        disabled = nlp.select_pipes(disable= ['lemmatizer', 'ner'])\n        for doc in nlp.pipe(cleaned_text, batch_size=1000, n_process=-1):\n            nouns = [token.text for token in doc if (token.pos_ in [\"NOUN\",\"PROPN\"])] \n            auxs =  [token.text for token in doc if (token.pos_ in [\"AUX\"])] \n            verbs =  [token.text for token in doc if (token.pos_ in [\"VERB\"])] \n            adjectives =  [token.text for token in doc if (token.pos_ in [\"ADJ\"])]        \n\n            noun_count.append(int(len(nouns)))\n            aux_count.append(int(len(auxs)))\n            verb_count.append(int(len(verbs)))\n            adj_count.append(int(len(adjectives)))\n        return np.transpose(np.vstack((noun_count, aux_count, verb_count, adj_count)))\n            \n        \n    def get_ner_features(self, cleaned_text):\n        nlp = spacy.load(self.spacy_model)\n        count_ner  = []\n        disabled = nlp.select_pipes(disable= ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer'])\n        for doc in nlp.pipe(cleaned_text, batch_size=1000, n_process=-1):\n            ners = [ent.label_ for ent in doc.ents] \n            count_ner.append(len(ners))\n        return np.array(count_ner).reshape(-1,1)   \n   \n    def get_count_features(self, cleaned_text):\n        list_count_words =[]\n        list_count_characters =[]\n        list_count_characters_no_space =[]\n        list_avg_word_length=[]\n        list_count_digits=[]\n        list_count_numbers=[]\n        for sent in cleaned_text:\n            words = re.sub(r'\\d+\\s','',sent)\n            numbers = re.findall(r'\\d+', sent)\n            #print(words)\n            #print(numbers)\n\n            count_word = len(words.split())\n            count_char = len(words)\n            count_char_no_space = len(''.join(words.split()))\n            avg_word_length = count_char_no_space/(count_word+1)\n            count_numbers = len(numbers)\n            count_digits = len(''.join(numbers))\n\n            list_count_words.append(count_word)\n            list_count_characters.append(count_char)\n            list_count_characters_no_space.append(count_char_no_space)\n            list_avg_word_length.append(avg_word_length)\n            list_count_digits.append(count_digits)\n            list_count_numbers.append(count_numbers)  \n            \n        count_features = np.vstack((list_count_words, list_count_characters,\n                                  list_count_characters_no_space, list_avg_word_length,\n                                  list_count_digits,list_count_numbers ))\n        return np.transpose(count_features)\n        \n \n         \n    def fit(self, X, y = None):\n        return self\n    \n    def transform(self, X,y=None):\n        try:\n            if str(type(X)) not in [\"<class 'list'>\",\"<class 'numpy.ndarray'>\"]:\n                raise Exception('Expected list or numpy array got {}'.format(type(X)))\n\n            \n            preprocessor1 = SpacyPreprocessor(model = 'en_core_web_sm', lammetize=False, lower = False, \n                                   remove_stop=False )\n            preprocessor2 = SpacyPreprocessor(model = 'en_core_web_sm', lammetize=False, lower = False, \n                                   remove_stop=False, remove_punct= False )\n            \n            feature_names =[]\n            if (self.pos_features or self.ner_features):\n                cleaned_x_count_ner_pos = preprocessor2.fit_transform(X)\n            \n            if self.count_features:\n                cleaned_x_count_features = preprocessor1.fit_transform(X)\n                count_features = self.get_count_features(cleaned_x_count_features)\n                feature_names.extend(['count_words', 'count_characters',\n                                  'count_characters_no_space', 'avg_word_length',\n                                  'count_digits','count_numbers'])\n\t\t\t\t\n            else:\n                count_features = np.empty(shape = (0, 0))\n                \n            if self.pos_features: \n                pos_features = self.get_pos_features(cleaned_x_count_ner_pos)\n                feature_names.extend(['noun_count', 'aux_count', 'verb_count', 'adj_count'])\n            else:\n                 pos_features = np.empty(shape = (0, 0))\n                \n            if self.ner_features: \n                ner_features =self.get_ner_features(cleaned_x_count_ner_pos)\n                feature_names.extend(['ner'])\n            else:\n                 ner_features = np.empty(shape = (0, 0))\n                \n            return np.hstack((count_features, ner_features, pos_features)), feature_names\n            \n\n        except Exception as error:\n            print('An exception occured: ' + repr(error))\n","metadata":{"id":"IsV7LP6F7iaO","execution":{"iopub.status.busy":"2022-02-24T17:24:50.002254Z","iopub.execute_input":"2022-02-24T17:24:50.002871Z","iopub.status.idle":"2022-02-24T17:24:50.024603Z","shell.execute_reply.started":"2022-02-24T17:24:50.002833Z","shell.execute_reply":"2022-02-24T17:24:50.023919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Final Pipeline ","metadata":{"id":"8BSiMky5rBFM"}},{"cell_type":"code","source":"# count examples in each class\ncounter = Counter(df['label'])\n# estimate scale_pos_weight value\nestimate = counter[0] / counter[1]\nprint('Estimate: %.3f' % estimate)","metadata":{"id":"_KW-2aVctmuS","outputId":"6b3f712c-0901-406a-c934-aff55b03df14","execution":{"iopub.status.busy":"2022-02-24T17:24:50.025775Z","iopub.execute_input":"2022-02-24T17:24:50.026144Z","iopub.status.idle":"2022-02-24T17:24:50.042616Z","shell.execute_reply.started":"2022-02-24T17:24:50.02611Z","shell.execute_reply":"2022-02-24T17:24:50.041663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_cleaned = SpacyPreprocessor(model = 'en_core_web_sm', remove_stop=True).transform(X_train)","metadata":{"id":"VwQtMboVCPoS","execution":{"iopub.status.busy":"2022-02-24T17:24:50.043755Z","iopub.execute_input":"2022-02-24T17:24:50.043993Z","iopub.status.idle":"2022-02-24T17:24:57.115012Z","shell.execute_reply.started":"2022-02-24T17:24:50.043965Z","shell.execute_reply":"2022-02-24T17:24:57.11407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"featurizer =  ManualFeatures(spacy_model='en_core_web_sm')","metadata":{"id":"On72o_XaCX_O","execution":{"iopub.status.busy":"2022-02-24T17:24:57.116173Z","iopub.execute_input":"2022-02-24T17:24:57.116385Z","iopub.status.idle":"2022-02-24T17:24:57.120847Z","shell.execute_reply.started":"2022-02-24T17:24:57.11636Z","shell.execute_reply":"2022-02-24T17:24:57.120023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_features, feature_names  = featurizer.fit_transform(X_train)","metadata":{"id":"g66ul8GOCbTS","execution":{"iopub.status.busy":"2022-02-24T17:24:57.122272Z","iopub.execute_input":"2022-02-24T17:24:57.122765Z","iopub.status.idle":"2022-02-24T17:25:15.333923Z","shell.execute_reply.started":"2022-02-24T17:24:57.122724Z","shell.execute_reply":"2022-02-24T17:25:15.332279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_final = pd.concat((pd.DataFrame(X_train_cleaned, columns =['cleaned_text']), \n                           pd.DataFrame(X_train_features, columns=feature_names)),axis =1)","metadata":{"id":"Xw4P9N6oCe4u","execution":{"iopub.status.busy":"2022-02-24T17:25:15.336193Z","iopub.execute_input":"2022-02-24T17:25:15.336499Z","iopub.status.idle":"2022-02-24T17:25:15.346507Z","shell.execute_reply.started":"2022-02-24T17:25:15.336446Z","shell.execute_reply":"2022-02-24T17:25:15.345576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_final.head()","metadata":{"id":"gvVNjFV8ChT_","outputId":"8728bf0e-d933-4c1b-a420-fe40b2f49929","execution":{"iopub.status.busy":"2022-02-24T17:25:15.348074Z","iopub.execute_input":"2022-02-24T17:25:15.34853Z","iopub.status.idle":"2022-02-24T17:25:15.374145Z","shell.execute_reply.started":"2022-02-24T17:25:15.3485Z","shell.execute_reply":"2022-02-24T17:25:15.373585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_final.info()","metadata":{"id":"rDtcTKLxClRG","outputId":"3b11901d-1829-4794-e6a0-f4bb5c6bcfcb","execution":{"iopub.status.busy":"2022-02-24T17:25:15.375185Z","iopub.execute_input":"2022-02-24T17:25:15.375568Z","iopub.status.idle":"2022-02-24T17:25:15.389527Z","shell.execute_reply.started":"2022-02-24T17:25:15.375538Z","shell.execute_reply":"2022-02-24T17:25:15.388658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.base import TransformerMixin, BaseEstimator\nfrom scipy.sparse import csr_matrix\nclass SparseTransformer(TransformerMixin, BaseEstimator):\n    \n  def __init__(self):\n    return None\n\n  def fit(self, X, y=None):\n      return self\n\n  def transform(self, X, y=None):\n      return csr_matrix(X)","metadata":{"id":"_4xlH_jCCmSL","execution":{"iopub.status.busy":"2022-02-24T17:25:15.390597Z","iopub.execute_input":"2022-02-24T17:25:15.391219Z","iopub.status.idle":"2022-02-24T17:25:15.395997Z","shell.execute_reply.started":"2022-02-24T17:25:15.391189Z","shell.execute_reply":"2022-02-24T17:25:15.395367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sparse_features = Pipeline([('sparse', SparseTransformer()),                     \n                        ]) \nvectorizer = Pipeline([('tfidf', TfidfVectorizer(max_features=5)),                     \n                        ]) ","metadata":{"id":"apz2P8-8CvQS","execution":{"iopub.status.busy":"2022-02-24T17:25:15.397236Z","iopub.execute_input":"2022-02-24T17:25:15.397872Z","iopub.status.idle":"2022-02-24T17:25:15.409149Z","shell.execute_reply.started":"2022-02-24T17:25:15.397838Z","shell.execute_reply":"2022-02-24T17:25:15.408524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sparse_features.fit_transform(X_train_final.iloc[:,1:])","metadata":{"id":"nEZIOeFECxtC","outputId":"4321552d-43b1-447b-da12-5b4adf0e2c4e","execution":{"iopub.status.busy":"2022-02-24T17:25:15.410482Z","iopub.execute_input":"2022-02-24T17:25:15.410731Z","iopub.status.idle":"2022-02-24T17:25:15.424533Z","shell.execute_reply.started":"2022-02-24T17:25:15.410702Z","shell.execute_reply":"2022-02-24T17:25:15.423816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vectorizer.fit_transform(X_train_final.iloc[:,0])","metadata":{"id":"nUHnycnWC13R","outputId":"067cb111-8310-4093-d2d8-0ff0e3d159c6","execution":{"iopub.status.busy":"2022-02-24T17:25:15.425915Z","iopub.execute_input":"2022-02-24T17:25:15.426648Z","iopub.status.idle":"2022-02-24T17:25:15.486225Z","shell.execute_reply.started":"2022-02-24T17:25:15.426575Z","shell.execute_reply":"2022-02-24T17:25:15.485659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.compose import ColumnTransformer, make_column_transformer\ncombined_features = ColumnTransformer(\n transformers=[\n     ('tfidf', vectorizer, 'cleaned_text'),\n      ], remainder=sparse_features\n     )","metadata":{"id":"CtcS_Fm8C5Ew","execution":{"iopub.status.busy":"2022-02-24T17:25:15.48752Z","iopub.execute_input":"2022-02-24T17:25:15.488344Z","iopub.status.idle":"2022-02-24T17:25:15.493299Z","shell.execute_reply.started":"2022-02-24T17:25:15.488302Z","shell.execute_reply":"2022-02-24T17:25:15.492736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classifier_1 = Pipeline([('combined_features',  combined_features),\n                         ('classifier', XGBClassifier(scale_pos_weight=estimate)),\n                        ])","metadata":{"id":"9G2QLkjUDGAn","execution":{"iopub.status.busy":"2022-02-24T17:25:15.494351Z","iopub.execute_input":"2022-02-24T17:25:15.495249Z","iopub.status.idle":"2022-02-24T17:25:15.504767Z","shell.execute_reply.started":"2022-02-24T17:25:15.495207Z","shell.execute_reply":"2022-02-24T17:25:15.503915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"param_grid_1 = {'combined_features__tfidf__tfidf__max_features': [500, 1000, 2000],\n                'combined_features__tfidf__tfidf__max_df': [0.6, 0.8, 1.0],\n                'combined_features__tfidf__tfidf__ngram_range': [(1,1), (1,2), (1,3)]}\n\ngrid_1 = GridSearchCV(estimator=classifier_1, param_grid=param_grid_1, \n                                 cv = 2,scoring = fscore, n_jobs= -1, verbose = 4 )","metadata":{"id":"7d-d5Ng6DJir","execution":{"iopub.status.busy":"2022-02-24T17:25:15.506117Z","iopub.execute_input":"2022-02-24T17:25:15.506446Z","iopub.status.idle":"2022-02-24T17:25:15.516874Z","shell.execute_reply.started":"2022-02-24T17:25:15.506408Z","shell.execute_reply":"2022-02-24T17:25:15.516092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid_1.fit(X_train_final, y_train)","metadata":{"id":"wJOXj_uqDYcI","outputId":"5512fe7e-73cc-488d-966e-da74ca13722a","execution":{"iopub.status.busy":"2022-02-24T17:25:15.518441Z","iopub.execute_input":"2022-02-24T17:25:15.518757Z","iopub.status.idle":"2022-02-24T17:40:24.70244Z","shell.execute_reply.started":"2022-02-24T17:25:15.518718Z","shell.execute_reply":"2022-02-24T17:40:24.701739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Best cross-validation score: {:.2f}\".format(grid_1.best_score_))\nprint(\"\\nBest parameters: \", grid_1.best_params_)\nprint(\"\\nBest estimator: \", grid_1.best_estimator_)","metadata":{"id":"sZC3ifZeDcQQ","outputId":"39c7488c-8ef1-40bc-b454-506476c5cd65","execution":{"iopub.status.busy":"2022-02-24T17:40:24.703732Z","iopub.execute_input":"2022-02-24T17:40:24.704577Z","iopub.status.idle":"2022-02-24T17:40:24.82057Z","shell.execute_reply.started":"2022-02-24T17:40:24.704537Z","shell.execute_reply":"2022-02-24T17:40:24.819661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"file_model = 'combined_model.pkl'","metadata":{"id":"L_JzAek2Dk9v","execution":{"iopub.status.busy":"2022-02-24T17:40:24.821707Z","iopub.execute_input":"2022-02-24T17:40:24.821912Z","iopub.status.idle":"2022-02-24T17:40:24.826062Z","shell.execute_reply.started":"2022-02-24T17:40:24.821887Z","shell.execute_reply":"2022-02-24T17:40:24.82518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"joblib.dump(grid_1.best_estimator_, file_model)","metadata":{"id":"W8PFGe_gDmEf","outputId":"4cc6b585-0aa8-4ee7-d47b-9e3c99e9e61e","execution":{"iopub.status.busy":"2022-02-24T17:40:24.827413Z","iopub.execute_input":"2022-02-24T17:40:24.827961Z","iopub.status.idle":"2022-02-24T17:40:25.013114Z","shell.execute_reply.started":"2022-02-24T17:40:24.827915Z","shell.execute_reply":"2022-02-24T17:40:25.012144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load the saved model\nloaded_model = joblib.load(file_model)","metadata":{"id":"LozdJ3bADqEH","execution":{"iopub.status.busy":"2022-02-24T17:40:25.015717Z","iopub.execute_input":"2022-02-24T17:40:25.015985Z","iopub.status.idle":"2022-02-24T17:40:25.125347Z","shell.execute_reply.started":"2022-02-24T17:40:25.015954Z","shell.execute_reply":"2022-02-24T17:40:25.124524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot learning curves\n# This cell can take upto 30 minutes to run\nplot_learning_curve(loaded_model, 'Learning Curves classifier_1', X_train_final, y_train)","metadata":{"id":"LZyRFulzDt3e","outputId":"eeda7e76-29d6-42a3-9e86-db52c968bac5","execution":{"iopub.status.busy":"2022-02-24T17:40:25.126886Z","iopub.execute_input":"2022-02-24T17:40:25.127104Z","iopub.status.idle":"2022-02-24T17:40:48.410354Z","shell.execute_reply.started":"2022-02-24T17:40:25.127079Z","shell.execute_reply":"2022-02-24T17:40:48.409536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Accuracy on Train data set\ngrid_classifier = loaded_model.score(X_train_final, y_train)\nprint(f'Accuracy on train set  is {grid_classifier}')","metadata":{"id":"7DaYiUaqDyt5","outputId":"8bc04ae9-cc95-414b-cdba-2e821f283ed1","execution":{"iopub.status.busy":"2022-02-24T17:40:48.411842Z","iopub.execute_input":"2022-02-24T17:40:48.412766Z","iopub.status.idle":"2022-02-24T17:40:48.507438Z","shell.execute_reply.started":"2022-02-24T17:40:48.412722Z","shell.execute_reply":"2022-02-24T17:40:48.506511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test_cleaned = SpacyPreprocessor(model = 'en_core_web_sm', remove_stop=True).transform(X_test)","metadata":{"id":"fV6WvUqsDzbU","execution":{"iopub.status.busy":"2022-02-24T17:40:48.50877Z","iopub.execute_input":"2022-02-24T17:40:48.509569Z","iopub.status.idle":"2022-02-24T17:40:51.539672Z","shell.execute_reply.started":"2022-02-24T17:40:48.509529Z","shell.execute_reply":"2022-02-24T17:40:51.538719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save  this to a file\nX_test_cleaned = 'x_test_cleaned_sparse_embed.pkl'","metadata":{"id":"FTe4RB_gD4uH","execution":{"iopub.status.busy":"2022-02-24T17:40:51.54502Z","iopub.execute_input":"2022-02-24T17:40:51.545259Z","iopub.status.idle":"2022-02-24T17:40:51.549503Z","shell.execute_reply.started":"2022-02-24T17:40:51.545231Z","shell.execute_reply":"2022-02-24T17:40:51.548552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"joblib.dump(X_test_cleaned, X_test_cleaned)","metadata":{"id":"cpXPq9RuD56U","outputId":"ecbe09fb-5653-4f07-e231-e3fdc26dd186","execution":{"iopub.status.busy":"2022-02-24T17:40:51.550818Z","iopub.execute_input":"2022-02-24T17:40:51.551303Z","iopub.status.idle":"2022-02-24T17:40:51.563539Z","shell.execute_reply.started":"2022-02-24T17:40:51.551258Z","shell.execute_reply":"2022-02-24T17:40:51.562697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Final Pipeline\ndef final_pipeline(text):\n  cleaned_text = SpacyPreprocessor(model = 'en_core_web_sm', remove_stop=True).transform(text)\n  X_features, feature_names  = featurizer.fit_transform(text)\n  X_final = pd.concat((pd.DataFrame(cleaned_text, columns =['cleaned_text']), \n                           pd.DataFrame(X_features, columns=feature_names)),axis =1)\n  \n  predictions = loaded_model.predict(X_final)\n  return predictions","metadata":{"id":"CxoPSYAhD8xG","execution":{"iopub.status.busy":"2022-02-24T17:40:51.565158Z","iopub.execute_input":"2022-02-24T17:40:51.565655Z","iopub.status.idle":"2022-02-24T17:40:51.573747Z","shell.execute_reply.started":"2022-02-24T17:40:51.565622Z","shell.execute_reply":"2022-02-24T17:40:51.572979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predicted values for Test data set\ny_test_pred = final_pipeline(X_test)","metadata":{"id":"WJyCpL68EDH6","execution":{"iopub.status.busy":"2022-02-24T17:40:51.574674Z","iopub.execute_input":"2022-02-24T17:40:51.575261Z","iopub.status.idle":"2022-02-24T17:41:03.429981Z","shell.execute_reply.started":"2022-02-24T17:40:51.575231Z","shell.execute_reply":"2022-02-24T17:41:03.429116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('\\nTest set classification report:\\n\\n',classification_report(y_test, y_test_pred ))","metadata":{"id":"cr7Lry3ZEHCa","outputId":"9fd4184e-91e9-4df9-f1f6-a1b71a307884","execution":{"iopub.status.busy":"2022-02-24T17:41:03.431363Z","iopub.execute_input":"2022-02-24T17:41:03.432275Z","iopub.status.idle":"2022-02-24T17:41:03.441992Z","shell.execute_reply.started":"2022-02-24T17:41:03.432237Z","shell.execute_reply":"2022-02-24T17:41:03.441374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('\\nTest set classification report:\\n\\n',fbeta_score(y_test, y_test_pred, beta=0.5))","metadata":{"id":"UGKnxAWZEJOs","outputId":"2b15ff40-0330-4b2d-cc61-04b87b7d034c","execution":{"iopub.status.busy":"2022-02-24T17:41:03.44286Z","iopub.execute_input":"2022-02-24T17:41:03.443231Z","iopub.status.idle":"2022-02-24T17:41:03.451027Z","shell.execute_reply.started":"2022-02-24T17:41:03.443188Z","shell.execute_reply":"2022-02-24T17:41:03.45024Z"},"trusted":true},"execution_count":null,"outputs":[]}]}