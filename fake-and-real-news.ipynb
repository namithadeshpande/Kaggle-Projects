{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1 style=\"text-align:center\">   \n      <font color = Black >\n                Fake and Real News \n        </font>    \n</h1>   \n<hr style=\"width:100%;height:5px;border-width:0;color:gray;background-color:gray\">\n<center><img style = \"height:450px;\" src=\"https://www.txstate.edu/cache78a0c25d34508c9d84822109499dee61/imagehandler/scaler/gato-docs.its.txstate.edu/jcr:21b3e33f-31c9-4273-aeb0-5b5886f8bcc4/fake-fact.jpg?mode=fit&width=1600\"></center>\n\n# Introduction\n\nThis table of contents gives an overview about different sections in the notebook.\n\n1. [Load Required Libraries](#1)\n2. [Import the Dataset](#2)\n3. [Exploratory Data Analysis](#3)\n4. [Data Cleaning](#4)\n    * [Removing Stopwords](#5)\n    * [Lemmatization](#6)\n    * [Word Cloud](#7)\n5. [N-gram Analysis](#8)  \n    * [Unigram Analysis](#9)\n    * [Bigram Analysis](#10)\n    * [Trigram Analysis](#11)\n6. [Modeling](#12)\n7. [Conclusion](#13)","metadata":{}},{"cell_type":"markdown","source":"<a id = \"1\" ></a>\n# Load Required Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom bs4 import BeautifulSoup\nimport re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer \nfrom wordcloud import WordCloud, STOPWORDS\nfrom nltk.tokenize import word_tokenize\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nfrom sklearn.feature_extraction.text import TfidfVectorizer","metadata":{"execution":{"iopub.status.busy":"2021-09-07T23:35:19.60626Z","iopub.execute_input":"2021-09-07T23:35:19.606826Z","iopub.status.idle":"2021-09-07T23:35:19.613791Z","shell.execute_reply.started":"2021-09-07T23:35:19.606781Z","shell.execute_reply":"2021-09-07T23:35:19.612987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"2\" ></a>\n# Import the Dataset\n\nThere are two datasets seperate for fake and real news. We will import them into the environment","metadata":{}},{"cell_type":"code","source":"#import dataset\nfake = pd.read_csv(\"../input/fake-and-real-news-dataset/Fake.csv\")\ntrue = pd.read_csv(\"../input/fake-and-real-news-dataset/True.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-09-07T23:35:19.615079Z","iopub.execute_input":"2021-09-07T23:35:19.615448Z","iopub.status.idle":"2021-09-07T23:35:20.809548Z","shell.execute_reply.started":"2021-09-07T23:35:19.615421Z","shell.execute_reply":"2021-09-07T23:35:20.808646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#data exploration\nfake.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-07T23:35:20.811299Z","iopub.execute_input":"2021-09-07T23:35:20.811629Z","iopub.status.idle":"2021-09-07T23:35:20.824804Z","shell.execute_reply.started":"2021-09-07T23:35:20.811583Z","shell.execute_reply":"2021-09-07T23:35:20.823673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"true.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-07T23:35:20.826427Z","iopub.execute_input":"2021-09-07T23:35:20.826748Z","iopub.status.idle":"2021-09-07T23:35:20.84618Z","shell.execute_reply.started":"2021-09-07T23:35:20.826711Z","shell.execute_reply":"2021-09-07T23:35:20.845086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The columns in the datasets are:\n* **title** - The title of the article\n* **text** - The text of the article\n* **subject** - The subject of the article\n* **date** - The date at which the article was posted\n\nThe dataset contains no target variable. We need to create manually and add it to the datasets. We will create a binary variable called label. The label variable will have '0' for real news and '1' for fake news. ","metadata":{}},{"cell_type":"code","source":"#adding label columns to both fake news and true news dataset\nfake[\"label\"] = 1\ntrue[\"label\"] = 0","metadata":{"execution":{"iopub.status.busy":"2021-09-07T23:35:20.847736Z","iopub.execute_input":"2021-09-07T23:35:20.848148Z","iopub.status.idle":"2021-09-07T23:35:20.854916Z","shell.execute_reply.started":"2021-09-07T23:35:20.848105Z","shell.execute_reply":"2021-09-07T23:35:20.854212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will combine the seperate datasets into one for our further analysis","metadata":{}},{"cell_type":"code","source":"#combining both the datasets into one\ndf = pd.concat([fake, true], ignore_index = True)\ndf","metadata":{"execution":{"iopub.status.busy":"2021-09-07T23:35:20.856121Z","iopub.execute_input":"2021-09-07T23:35:20.85662Z","iopub.status.idle":"2021-09-07T23:35:20.881691Z","shell.execute_reply.started":"2021-09-07T23:35:20.856568Z","shell.execute_reply":"2021-09-07T23:35:20.88071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"3\" ></a>\n# Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"#EDA\n#checking for missing values in the combined dataset\ndf.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-09-07T23:35:20.884061Z","iopub.execute_input":"2021-09-07T23:35:20.884354Z","iopub.status.idle":"2021-09-07T23:35:20.913791Z","shell.execute_reply.started":"2021-09-07T23:35:20.884326Z","shell.execute_reply":"2021-09-07T23:35:20.912608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are no null/missing values in the dataset.","metadata":{}},{"cell_type":"code","source":"#checking for imbalance in the dataset\ncount = df['label'].value_counts().values\nsns.barplot(x = [0,1], y = count)\nplt.title('Target variable count')","metadata":{"execution":{"iopub.status.busy":"2021-09-07T23:35:20.915916Z","iopub.execute_input":"2021-09-07T23:35:20.916513Z","iopub.status.idle":"2021-09-07T23:35:21.043628Z","shell.execute_reply.started":"2021-09-07T23:35:20.916466Z","shell.execute_reply":"2021-09-07T23:35:21.042573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the plot above, you can see there is no class imbalance in the target variable. We have almost equal instances for negative class (\"0\" - Real) and the class of interest (\"1\" - Fake).","metadata":{}},{"cell_type":"code","source":"#distribution of fake and real news among subjects\nplt.figure(figsize=(12,8))\nsns.countplot(x = \"subject\", data=df, hue = \"label\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-07T23:35:21.045182Z","iopub.execute_input":"2021-09-07T23:35:21.04565Z","iopub.status.idle":"2021-09-07T23:35:21.332447Z","shell.execute_reply.started":"2021-09-07T23:35:21.045605Z","shell.execute_reply":"2021-09-07T23:35:21.331479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"4\" ></a>\n# Data Cleaning\nWe will begin with the preprocessing steps before the text is fed into the model for prediction. ","metadata":{}},{"cell_type":"code","source":"#data cleaning\n#combining the title and text columns\ndf['text'] = df['title'] + \" \" + df['text']\n#deleting few columns from the data \ndel df['title']\ndel df['subject']\ndel df['date']","metadata":{"execution":{"iopub.status.busy":"2021-09-07T23:35:21.333671Z","iopub.execute_input":"2021-09-07T23:35:21.333979Z","iopub.status.idle":"2021-09-07T23:35:21.497244Z","shell.execute_reply.started":"2021-09-07T23:35:21.333949Z","shell.execute_reply":"2021-09-07T23:35:21.496267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"5\" ></a>\n## Removing stopwords\nOne of the major forms of pre-processing is to filter out useless data. In NLP, useless words, are referred to as stop words. We will use the `nltk` library for this purpose. This is how we are making our processed content more efficient by removing words that do not contribute to any future operations.","metadata":{}},{"cell_type":"code","source":"#choosing the language as english\nstop = set(stopwords.words('english'))\n#removing punctuation marks\npunctuation = list(string.punctuation)\n#adding punctuations to the list of stop words \nstop.update(punctuation)\n\n#Removing the square brackets\ndef remove_brackets(text):\n    return re.sub('\\[[^]]*\\]', '', text)\n\n# Removing URL's\ndef remove_urls(text):\n    return re.sub(r'http\\S+', '', text)\n\n#Removing the stopwords from text\ndef remove_stopwords(text):\n    final_text = []\n    text = text.lower()\n    for i in text.split():\n        if i.strip() not in stop:\n            final_text.append(i.strip())\n    return \" \".join(final_text)\n\n#Removing the noisy text\ndef clean_text(text):\n    text = remove_brackets(text)\n    text = remove_urls(text)\n    text = remove_stopwords(text)\n    return text\n\n#Apply function on text column\ndf['text']=df['text'].apply(clean_text)\ndf['text']","metadata":{"execution":{"iopub.status.busy":"2021-09-07T23:35:21.498741Z","iopub.execute_input":"2021-09-07T23:35:21.499158Z","iopub.status.idle":"2021-09-07T23:35:28.746751Z","shell.execute_reply.started":"2021-09-07T23:35:21.499118Z","shell.execute_reply":"2021-09-07T23:35:28.745637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"6\" ></a>\n## Lemmatization\nThe next step is to perform Lemmatization. It is the process of converting a word to its base form. For example: 'Caring' -> 'Care'; 'hanging' -> 'hang'","metadata":{}},{"cell_type":"code","source":"#lemmatization\n# Init the Wordnet Lemmatizer\nlemmatizer = WordNetLemmatizer()\n\n#A function which takes a sentence/corpus and gets its lemmatized version.\ndef lemmatize_text(text):\n    token_words=word_tokenize(text) \n#we need to tokenize the sentence or else lemmatizing will return the entire sentence as is.\n    lemma_sentence=[]\n    for word in token_words:\n        lemma_sentence.append(lemmatizer.lemmatize(word))\n        lemma_sentence.append(\" \")\n    return \"\".join(lemma_sentence)\n\n#Apply function on text column\ndf['text']=df['text'].apply(lemmatize_text)\ndf","metadata":{"execution":{"iopub.status.busy":"2021-09-07T23:35:28.748281Z","iopub.execute_input":"2021-09-07T23:35:28.74873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"7\" ></a>\n## Word Cloud\n### Fake News Word Cloud\n","metadata":{}},{"cell_type":"code","source":"#word cloud for fake news\ncloud = WordCloud(max_words = 500, stopwords = STOPWORDS, background_color = \"white\").generate(\" \".join(df[df.label == 1].text))\nplt.figure(figsize=(40, 30))\nplt.imshow(cloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.tight_layout(pad=0)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Real News Word Cloud","metadata":{}},{"cell_type":"code","source":"#word cloud for real news\ncloud = WordCloud(max_words = 500, stopwords = STOPWORDS, background_color = \"white\").generate(\" \".join(df[df.label == 0].text))\nplt.figure(figsize=(40, 30))\nplt.imshow(cloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.tight_layout(pad=0)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"8\" ></a>\n# N-gram Analysis","metadata":{}},{"cell_type":"code","source":"#finding n-grams\ntexts = ''.join(str(df['text'].tolist()))\n\n# first get individual words\ntokenized = texts.split()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"9\" ></a>\n## Unigram Analysis","metadata":{}},{"cell_type":"code","source":"#unigram\nunigram = (pd.Series(nltk.ngrams(tokenized, 1)).value_counts())[:20]\nunigram.sort_values().plot.barh(width=.9, figsize=(12, 8))\nplt.title('20 Most Frequently Occuring Unigrams')\nplt.ylabel('Unigram')\nplt.xlabel('# of Occurances')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"10\" ></a>\n## Bigram Analysis","metadata":{}},{"cell_type":"code","source":"#bigrams\nbigram = (pd.Series(nltk.ngrams(tokenized, 2)).value_counts())[:20]\nbigram.sort_values().plot.barh(width=.9, figsize=(12, 8))\nplt.title('20 Most Frequently Occuring Bigrams')\nplt.ylabel('Bigram')\nplt.xlabel('# of Occurances')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"11\" ></a>\n## Trigram Analysis","metadata":{}},{"cell_type":"code","source":"#trigrams\ntrigram = (pd.Series(nltk.ngrams(tokenized, 3)).value_counts())[:20]\ntrigram.sort_values().plot.barh(width=.9, figsize=(12, 8))\nplt.title('20 Most Frequently Occuring Trigrams')\nplt.ylabel('Trigram')\nplt.xlabel('# of Occurances')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"12\" ></a>\n# Modeling\nIn this step, I am making use of various Classification models for prediction. The models use cleaned text data for analysis.\n\n#### Using TF-IDF Vectorizer\nThis is an acronym than stands for \"Term Frequency â€“ Inverse Document Frequency\" which are the components of the resulting scores assigned to each word.The TfidfVectorizer will tokenize documents, learn the vocabulary and inverse document frequency weightings, and allow you to encode new documents.","metadata":{}},{"cell_type":"code","source":"#modeling\ndef get_prediction(vectorizer, classifier, X_train, X_test, y_train, y_test):\n    pipe = Pipeline([('vector', vectorizer),\n                    ('model', classifier)])\n    model = pipe.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    print(\"Accuarcy: {}\".format(round(accuracy_score(y_test, y_pred)*100,2)))\n    cm = confusion_matrix(y_test, y_pred)\n    print(\"Confusion Matrix: \\n\", cm)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#pipeline implementation\nX_train, X_test, y_train, y_test = train_test_split(df['text'], df['label'], test_size = 0.3, random_state= 0)\nclassifiers = [LogisticRegression(),KNeighborsClassifier(n_neighbors=5), DecisionTreeClassifier(), GradientBoostingClassifier(), \n               RandomForestClassifier()]\nfor classifier in classifiers:\n    print(\"\\n\\n\", classifier)\n    get_prediction(TfidfVectorizer(), classifier, X_train, X_test, y_train, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"13\" ></a>\n# Conclusion\nDecision Tree, Gradient Boosting and Random Forest Algorithms are giving an accuracy above 99% which is a really good score. There might be chances of overfitting which can be explored using validation curve. I will explore overfitting furthur. \n\n**Upvote if you like this notebook. Happy Learning!**","metadata":{}}]}